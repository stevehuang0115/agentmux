# Research Brief: AI Agent Framework Comparison

## Objective

Produce a comprehensive competitive analysis comparing the leading AI agent frameworks. The final deliverable is a report that helps engineering leadership decide which framework to adopt or build against.

## Frameworks to Compare

1. **LangChain / LangGraph** — Python-first orchestration framework
2. **CrewAI** — Multi-agent framework with role-based collaboration
3. **AutoGen / AG2** — Microsoft's multi-agent conversation framework
4. **OpenAI Agents SDK** — OpenAI's native agent framework
5. **Semantic Kernel** — Microsoft's enterprise AI orchestration SDK

## Research Dimensions

For each framework, gather data on:

- **Architecture**: Single-agent vs multi-agent, orchestration model, communication patterns
- **Language support**: Python, TypeScript/JS, other SDKs
- **Tool ecosystem**: Built-in tools, MCP support, custom tool authoring
- **Memory & state**: Conversation history, long-term memory, state persistence
- **Deployment**: Self-hosted, cloud, Docker, serverless options
- **Community**: GitHub stars, contributors, release cadence, Discord/forum activity
- **Documentation**: Quality, examples, tutorials, API reference completeness
- **Enterprise readiness**: Security, compliance, SSO, audit logging

## Deliverables

1. **Comparison matrix** (`output/comparison-matrix.md`) — Feature-by-feature table
2. **Gap analysis** (`output/gap-analysis.md`) — Where each framework falls short
3. **Final report** (`output/report.md`) — Executive summary, findings, recommendations

## Timeline

This is a focused research sprint. The team should complete all deliverables in a single session.

## Notes

- Prioritize accuracy over comprehensiveness — it's better to have verified facts than broad guesses
- Include links to sources where possible
- Flag any areas where information is uncertain or rapidly changing
